{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The novelty about BERT is the attention model built-in. Attention is a complex task, and it requires different-\n",
    "tasks working in conjunction. Seq2seq is a model that takes a sequence of words and creates an output of another-\n",
    "sequence of words. The mechanics of seq2seq is an encoder-decoder model in which the information is captured-\n",
    "into a “context vector”; the encoder reduces the dimensionality of the model. The encoder and decoder are-\n",
    "recursive neural networks because RNNs have a longer memory; for newer NLP models, RNNs tend to be bidirectional too,-\n",
    "which allows the model to capture deeper intrinsic correlations. Most common size of context vectors are: 256,512,1024.\n",
    "\n",
    "The input of an encoder is one tokenized word along with one hidden state. When the two inputs get processed in the-\n",
    "encoder, the encoder generates an output, and the next hidden state is used as an input on the following input for-\n",
    "the enconder. In the same way, the encoder encodes data representations, and the decoder unrolls data representation-\n",
    "into a sequence output.\n",
    "\n",
    "To make the model “attentive,” this encoder-decoder is modified so the model can pay attention to important parts of-\n",
    "the input sequence. The main difference is that the encoder passes all the hidden states to the decoder, not just one-\n",
    "at a time.\n",
    "\n",
    "In the process of decoding,  each encoder hidden state is associated with specific words in the input sequence; each-\n",
    "hidden state gets a “score” inferred by a softmax function, each hidden score is multiplied by its score and this-\n",
    "amplifies the hidden states, while the ones with the more minor scores get minimized, in a vanishing gradient type.-\n",
    "How the model achieves attention is by masking a relevant word of the text and the model will infer it, based on the-\n",
    "score, it knows what “attention vector” the sentence belongs to."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fuzzy-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "\n",
    "\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig,map_stock_config_to_params,load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-amplifier",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/praveengovi/classify-emotions-in-text-with-bert\n",
    "\n",
    "The dataset contains 5 different emotions: ['sadness', 'anger', 'love', 'surprise', 'fear', 'joy'].\n",
    "\n",
    "Dataset is already processed but the emotions( labels) must be changed to numerical values-\n",
    "since this is a classification task. Pandas framework is used to load data and process it-\n",
    "into different emotions. {'joy':0,'sadness':1,'anger':2,'fear':3,'love':4,'surprise':5}-\n",
    "Once the labels are changed, the object must be changed to a int32 because this is the-\n",
    "encoding that the network used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "useful-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('D:/bert-fine-tune-tf2/data/pre-processed-data/train.txt',header=None, sep=';')\n",
    "test_df = pd.read_csv('D:/bert-fine-tune-tf2/data\\pre-processed-data/test.txt',header=None, sep=';')\n",
    "\n",
    "train_df.columns=['sentence', 'emotion']\n",
    "test_df.columns=['sentence','emotion']\n",
    "\n",
    "train_df=train_df.replace({'emotion': {'joy':0,'sadness':1,'anger':2,'fear':3,'love':4,'surprise':5}})\n",
    "test_df=test_df.replace({'emotion': {'joy':0,'sadness':1,'anger':2,'fear':3,'love':4,'surprise':5}})\n",
    "\n",
    "train_df[\"emotion\"]=train_df['emotion'].astype('int32')\n",
    "test_df[\"emotion\"]=test_df['emotion'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-teach",
   "metadata": {},
   "source": [
    "For train and test data, the data frame is split in 90/10 and result is two datasets-\n",
    "of size train {18,000 by 2} and {2,000 by 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "remarkable-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('D:/bert-fine-tune-tf2/data/processed-data/train.csv', sep=',',index=False, header=False)\n",
    "test_df.to_csv('D:/bert-fine-tune-tf2/data/processed-data/test.csv', sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-technical",
   "metadata": {},
   "source": [
    "Hugging face offers pre-train models trained in the english language, there are different architectures-\n",
    "to choose from. Each language transformer has its own Tokenizer; Tokenizing is the process of mapping-\n",
    "words to numerical values; bert uses an attention model to understand context by masking a word for-\n",
    "inference. The dataset to be used must be Tokenized with the corresponding tokenizer according to -\n",
    "the model to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "regulation-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= FullTokenizer(vocab_file='D:/bert-fine-tune-tf2/architecture-uncased_L-24_H-1024_A-16/vocab.txt',do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-craft",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each transformer model folder comes with 3 files:\n",
    "\n",
    "The model is the architecture used to train this model which must be loaded to fine-tune on a particula-\n",
    "dataset and the shape is the following:\n",
    "\n",
    "{\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 1024,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 4096,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"num_attention_heads\": 16,\n",
    "  \"num_hidden_layers\": 24,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 30522\n",
    "}\n",
    "The second file is the checkpoints file which is a filer that saves all parameter related to the model.\n",
    "Checkpoints is different from the actual model because the model is the architecture while the checkpoints-\n",
    "are hyper parameter and tensors created during the model training process and start-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "bert_model='D:/bert-fine-tune-tf2/architecture-uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16'\n",
    "bert_ckpt_file = 'D:/bert-fine-tune-tf2/architecture-uncased_L-24_H-1024_A-16/bert_model.ckpt'\n",
    "bert_config_file = 'D:/bert-fine-tune-tf2/architecture-uncased_L-24_H-1024_A-16/bert_config.json'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using transformers\n",
    "Transformers is a method of training a model in a stack architecture, encoders also run in parallel-\n",
    "with decoders and this model boosts speed in training. Each decoder and encoder has six elements,-\n",
    "this is the model published but it can be more than 6.  The input of the encoder flows upstream and-\n",
    "goes across all 6 encoders and each one of these encoders have 2 layers inside, a feed forward neural-\n",
    "network and a self-attention component.\n",
    "\n",
    "The following code takes two dataset: train and test and tokenizes the samples; it also adds \"CLS\"and \"SEP\"-\n",
    "this is required for the model to know the beginning and ending of the phrase. We must limit input lenght-\n",
    "to be 192 characters; since all the samples must be of the same lenght, if a sample is shorter, we will use-\n",
    "padding to add zeroes and fill empty characters with zeroes until it reaches 192 characters.\n",
    "The returning object will be all integers of encoding int32."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "stupid-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDetectionData:\n",
    "    DATA_COLUMN = \"sentence\"\n",
    "    LABEL_COLUMN = \"emotion\"\n",
    "\n",
    "    def __init__(self, train_df, test_df, tokenizer: FullTokenizer, classes, max_seq_len=192):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = 0\n",
    "        self.classes = classes\n",
    "\n",
    "        ((self.train_x, self.train_y), (self.test_x, self.test_y)) =\\\n",
    "            map(self._prepare, [train_df, test_df])\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text, label = row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "            tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            self.max_seq_len = max(self.max_seq_len, len(tokens_ids))\n",
    "            x.append(tokens_ids)\n",
    "            y.append(self.classes.index(label))\n",
    "        return np.array(x), np.array(y)\n",
    "        \n",
    "    def _pad(self,ids):\n",
    "        x=[]\n",
    "        for input_ids in ids:\n",
    "            input_ids=input_ids[:min(len(input_ids),self.max_seq_len-2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))          \n",
    "            x.append(np.array(input_ids))\n",
    "        return np.array(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Tensorflow 2 implementation was taken from this repo:\n",
    "\n",
    "https://github.com/kpe/bert-for-tf2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "supported-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, bert_ckpt_file):\n",
    "\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert_params.adapter_size = None\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "  input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n",
    "  bert_output = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", bert_output.shape)\n",
    "\n",
    "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "  cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "  logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "  logits = keras.layers.Dropout(0.5)(logits)\n",
    "  logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n",
    "\n",
    "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "        \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "rubber-start",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\bert-fine-tune-tf2\\venv\\lib\\site-packages\\ipykernel_launcher.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "classes=train_df.emotion.unique().tolist()\n",
    "data = IntentDetectionData(train_df, test_df, tokenizer, classes, max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "under-script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 87, 1024)\n",
      "Done loading 388 BERT weights from: D:/bert-fine-tune-tf2/architecture-uncased_L-24_H-1024_A-16/bert_model.ckpt into <bert.model.BertModelLayer object at 0x0000024A872A3B88> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n"
     ]
    }
   ],
   "source": [
    "model = create_model(data.max_seq_len, bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "architectural-honolulu",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}